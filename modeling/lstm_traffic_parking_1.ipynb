{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "import keras\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects: run the code on lstm_traffic_only_1 to generate the csv info\n",
    "traffic_df = pd.read_csv(\"../data/traffic_pre_lstm.csv\")\n",
    "traffic_df['timestamp'] = pd.to_datetime(traffic_df['timestamp'])\n",
    "traffic_df.set_index('timestamp', inplace=True)\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_training_set(training_sample, seq_length, pred_horizon, input_features, output_features):\n",
    "    seq_arrays = []\n",
    "    seq_labs = []\n",
    "\n",
    "    for i in range(training_sample.shape[0] - seq_length - pred_horizon):\n",
    "        seq_arrays.append(\n",
    "            training_sample.iloc[i:seq_length+i, :input_features].to_numpy())\n",
    "\n",
    "        seq_labs.append(\n",
    "            training_sample.iloc[seq_length+pred_horizon+i, :output_features])\n",
    "\n",
    "    seq_arrays = np.array(seq_arrays, dtype=object).astype(np.float32)\n",
    "    seq_labs = np.array(seq_labs, dtype=object).astype(np.float32)\n",
    "\n",
    "    return seq_arrays, seq_labs\n",
    "\n",
    "# create validation dataset\n",
    "\n",
    "\n",
    "def create_validation_set(validation_sample, seq_length, pred_horizon, input_features, output_features):\n",
    "    val_arrays = []\n",
    "    val_labs = []\n",
    "\n",
    "    for i in range(validation_sample.shape[0] - seq_length - pred_horizon):\n",
    "        if i < seq_length:\n",
    "            val_arrays.append(\n",
    "                validation_sample.iloc[:(i+1), :input_features].to_numpy())\n",
    "\n",
    "            val_labs.append(validation_sample.iloc[:(\n",
    "                i+pred_horizon+1), :output_features].to_numpy()[-1])\n",
    "        else:\n",
    "            val_arrays.append(\n",
    "                validation_sample.iloc[i:seq_length+i, :input_features].to_numpy())\n",
    "\n",
    "            val_labs.append(\n",
    "                validation_sample.iloc[seq_length+i+pred_horizon, :output_features])\n",
    "\n",
    "    val_arrays = pad_sequences(\n",
    "        val_arrays, padding='pre', dtype=object).astype(np.float32)\n",
    "\n",
    "    val_labs = np.array(val_labs, dtype=object).astype(np.float32)\n",
    "\n",
    "    return val_arrays, val_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(model_path, \n",
    "                     seq_length, \n",
    "                     seq_arrays, \n",
    "                     seq_labs, \n",
    "                     input_features, \n",
    "                     output_features):\n",
    "    \n",
    "    # build the network\n",
    "    output_size = output_features\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_shape=(seq_length, input_features),\n",
    "        units=100, activation=\"relu\", return_sequences=True))\n",
    "    model.add(Dropout(0.025))\n",
    "\n",
    "    model.add(LSTM(units=64, activation=\"relu\", return_sequences=True))\n",
    "    model.add(Dropout(0.025))\n",
    "    model.add(LSTM(units=32, activation=\"relu\", return_sequences=False))\n",
    "    model.add(Dense(units=output_size, activation=\"linear\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer, metrics=['mse'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the network\n",
    "    history = model.fit(seq_arrays,\n",
    "                        seq_labs,\n",
    "                        epochs=1000,\n",
    "                        batch_size=80,\n",
    "                        validation_split=0.05,\n",
    "                        verbose=2,\n",
    "                        callbacks=[\n",
    "                            keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                min_delta=0.1,\n",
    "                                patience=5,\n",
    "                                verbose=0,\n",
    "                                mode='min'),\n",
    "                            keras.callbacks.ModelCheckpoint(\n",
    "                                model_path,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                mode='min',\n",
    "                                verbose=0)\n",
    "                        ])\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_across_network(model, val_arrays, val_labs, title):\n",
    "    y_pred_test = model.predict(val_arrays)\n",
    "    y_true_test = val_labs\n",
    "\n",
    "    # aggregating for easier visualization\n",
    "    y_pred_dv = [row.sum() for row in y_pred_test]\n",
    "    y_true_dv = [row.sum() for row in y_true_test]\n",
    "\n",
    "    start = 100\n",
    "    ts = 1000\n",
    "\n",
    "    plt.plot(y_pred_dv[start:start+ts], label='Predicted Value')\n",
    "    plt.plot(y_true_dv[start:start+ts], label='Actual Value')\n",
    "    plt.title(title,\n",
    "            fontsize=22, fontweight='bold')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('row')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 3: Parking information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_df = pd.read_csv(\"../data/aarhus_parking_geolocated.csv\")\n",
    "parking_df['updatetime'] = pd.to_datetime(parking_df['updatetime'])\n",
    "\n",
    "# create a timestamp at the 5 minute interval in line with the traffic data\n",
    "parking_df[\"timestamp\"] = parking_df[\"updatetime\"].apply(\n",
    "    pd.Timestamp.ceil, freq=\"5min\")\n",
    "parking_df.set_index(\"timestamp\", drop=True, inplace=True)\n",
    "\n",
    "# aggregate over the timestamp\n",
    "parking_df = parking_df.groupby(parking_df.index).agg({\n",
    "    'vehiclecount': 'sum',\n",
    "    'totalspaces': 'sum'\n",
    "})\n",
    "\n",
    "parking_df['occupancy_rate'] = parking_df['vehiclecount'] / \\\n",
    "    parking_df['totalspaces']\n",
    "\n",
    "# merge with a complete timestamp index\n",
    "timestamp_index = pd.DataFrame({\"timestamp\": pd.date_range(pd.to_datetime(\n",
    "    parking_df.index.min()), pd.to_datetime(parking_df.index.max()), freq=\"5min\").to_list()})\n",
    "\n",
    "parking_df = timestamp_index.merge(\n",
    "    parking_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"timestamp\",\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "# impute missing values using linear interpolation\n",
    "parking_df['occupancy_rate'].interpolate(\"linear\", inplace=True)\n",
    "parking_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "parking_features = ['occupancy_rate']\n",
    "\n",
    "combined_parking_traffic = traffic_df.merge(\n",
    "    parking_df[parking_features],\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "# remove rows at the end without occupancy data\n",
    "combined_parking_traffic = combined_parking_traffic[combined_parking_traffic[\"occupancy_rate\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_parking_traffic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample = combined_parking_traffic[100:17000]\n",
    "validation_sample = combined_parking_traffic[17001:]\n",
    "\n",
    "validation_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set predictive horizon and sequence length\n",
    "ph = 5\n",
    "seq_length = 12\n",
    "\n",
    "# features to randomly sample without replacement\n",
    "# must be a value between 1 to 450, inclusive\n",
    "input_features = 450\n",
    "output_features = 449\n",
    "\n",
    "sensor = pd.Series(training_sample.columns).sample(\n",
    "    input_features, replace=False).sort_values().to_list()\n",
    "\n",
    "\n",
    "seq_arrays, seq_labs = create_sequence_training_set(\n",
    "    training_sample, seq_length, ph, input_features, output_features)\n",
    "\n",
    "val_arrays, val_labs = create_validation_set(\n",
    "    validation_sample, seq_length, ph, input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_lstm_model('lstm_traffic_parking.keras',\n",
    "                                  seq_length,\n",
    "                                  seq_arrays,\n",
    "                                  seq_labs,\n",
    "                                  input_features,\n",
    "                                  output_features)\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history_loss(history, title):\n",
    "    # summarize history for Loss/MSE\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "summarize_history_loss(history, \"Traffic + Parking model loss/mse\")\n",
    "\n",
    "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
    "print('\\nMSE: {}'.format(scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_across_network(model, val_arrays, val_labs, 'Sum of Average Speed Across Network')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usd_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
