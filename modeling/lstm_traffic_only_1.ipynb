{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from typing import List\n",
    "\n",
    "# modeling\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_files(archive_path: str, headers: List[str]) -> pd.DataFrame:\n",
    "    if archive_path.endswith(\".tar.gz\"):\n",
    "        with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "            output = pd.concat([pd.read_csv(tar.extractfile(file), encoding=\"latin-1\")\n",
    "                               for file in tar.getnames()])\n",
    "\n",
    "    if archive_path.endswith(\".zip\"):\n",
    "        zip_file = ZipFile(archive_path)\n",
    "\n",
    "        output = pd.concat([pd.read_csv(zip_file.open(csv_file), header=None)\n",
    "                            for csv_file in zip_file.namelist()])\n",
    "\n",
    "    if headers:\n",
    "        output.columns = headers\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_file_archives = [\"../data/citypulse_traffic_raw_data_aarhus_aug_sep_2014.tar.gz\",\n",
    "                         \"../data/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip\",\n",
    "                         ]\n",
    "\n",
    "headers = [\"status\", \"avg_measured_time\", \"avg_speed\",\t\"ext_id\",\n",
    "           \"median_measured_time\", \"timestamp\", \"vehicle_count\", \"_id\", \"report_id\"]\n",
    "\n",
    "traffic_data = pd.concat([extract_csv_files(archive, headers)\n",
    "                          for archive in traffic_file_archives])\n",
    "\n",
    "traffic_meta_data = pd.read_csv(\"../data/trafficMetaData.csv\")\n",
    "\n",
    "traffic_data = traffic_data.merge(traffic_meta_data,\n",
    "                                  how=\"left\",\n",
    "                                  left_on=\"report_id\",\n",
    "                                  right_on=\"REPORT_ID\")\n",
    "\n",
    "traffic_data[\"timestamp\"] = pd.to_datetime(traffic_data[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data for LSTM input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_index = pd.DataFrame({\"timestamp\": pd.date_range(pd.to_datetime(\n",
    "    traffic_data[\"timestamp\"].min()), pd.to_datetime(traffic_data[\"timestamp\"].max()), freq=\"5min\").to_list()})\n",
    "\n",
    "ext_ids = np.sort(traffic_data[\"ext_id\"].unique())\n",
    "\n",
    "ts_df = pd.pivot_table(traffic_data[[\"timestamp\", \"ext_id\", \"avg_speed\"]],\n",
    "                       columns=\"ext_id\",\n",
    "                       index=\"timestamp\",\n",
    "                       values=\"avg_speed\",\n",
    "                       dropna=False)\n",
    "\n",
    "ts_df = timestamp_index.merge(ts_df, how=\"left\", left_on=\"timestamp\",\n",
    "                              right_index=True).set_index(\"timestamp\", drop=True)\n",
    "\n",
    "ts_df.interpolate(inplace=True)\n",
    "ts_df = ts_df.dropna()\n",
    "\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.shape\n",
    "ts_df.to_csv(\"../data/traffic_pre_lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample = ts_df[100:20000]\n",
    "validation_sample = ts_df[20001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences of training data\n",
    "\n",
    "# set predictive horizon and sequence length\n",
    "ph = 5\n",
    "seq_length = 12\n",
    "\n",
    "# features to randomly sample without replacement\n",
    "# must be a value between 1 to 449, inclusive \n",
    "features = 449 \n",
    "\n",
    "sensor = pd.Series(training_sample.columns).sample(\n",
    "    features, replace=False).sort_values().to_list()\n",
    "\n",
    "print(sensor)\n",
    "\n",
    "seq_arrays = []\n",
    "seq_labs = []\n",
    "\n",
    "training_features = training_sample[sensor]\n",
    "\n",
    "for i in range(training_features.shape[0]-seq_length-ph):\n",
    "    seq_arrays.append(training_features.iloc[i:seq_length+i].to_numpy())\n",
    "    seq_labs.append(training_features.iloc[seq_length+ph+i])\n",
    "\n",
    "seq_arrays = np.array(seq_arrays, dtype=object).astype(np.float32)\n",
    "seq_labs = np.array(seq_labs, dtype=object).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "val_arrays = []\n",
    "val_labs = []\n",
    "\n",
    "validation_features = validation_sample[sensor]\n",
    "\n",
    "for i in range(validation_features.shape[0]-seq_length-ph):\n",
    "    if i < 12:\n",
    "        val_arrays.append(validation_features.iloc[:(i+1)].to_numpy())\n",
    "        val_labs.append(validation_features.iloc[:(i+ph+1)].to_numpy()[-1])\n",
    "    else:\n",
    "        val_arrays.append(validation_features.iloc[i:seq_length+i].to_numpy())\n",
    "        val_labs.append(validation_features.iloc[seq_length+i+ph])\n",
    "\n",
    "val_arrays = pad_sequences(val_arrays, padding='pre',\n",
    "                           dtype=object).astype(np.float32)\n",
    "\n",
    "val_labs = np.array(val_labs, dtype=object).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to save model\n",
    "model_path = 'lstm_traffic_only_1.keras'\n",
    "\n",
    "# build the network\n",
    "features = len(sensor)\n",
    "output_size = features\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_shape=(seq_length, features),\n",
    "    units=64,\n",
    "    activation=\"relu\",\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.025))\n",
    "\n",
    "model.add(LSTM(\n",
    "    units=32,\n",
    "    activation=\"relu\",\n",
    "    return_sequences=True))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=16,\n",
    "          activation=\"relu\",\n",
    "          return_sequences=False))\n",
    "\n",
    "model.add(Dense(units=output_size, activation=\"relu\"))\n",
    "model.add(Dense(units=output_size, activation=\"relu\"))\n",
    "model.add(Dense(units=output_size, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(units=output_size, activation=\"linear\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_arrays,\n",
    "                    seq_labs,\n",
    "                    epochs=1000,\n",
    "                    batch_size=500,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=2,\n",
    "                    callbacks=[\n",
    "                        keras.callbacks.EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.1,\n",
    "                            patience=5,\n",
    "                            verbose=0,\n",
    "                            mode='min'),\n",
    "                        keras.callbacks.ModelCheckpoint(\n",
    "                            model_path,\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                            mode='min',\n",
    "                            verbose=0)\n",
    "                    ])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for Loss/MSE\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss/MSE')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
    "print('\\nMSE: {}'.format(scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(val_arrays)\n",
    "y_true_test = val_labs\n",
    "\n",
    "# aggregating for easier visualization\n",
    "y_pred_dv = [row.sum() for row in y_pred_test]\n",
    "y_true_dv = [row.sum() for row in y_true_test]\n",
    "\n",
    "start = 5000\n",
    "ts = 1000\n",
    "\n",
    "fig_verify = plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_pred_dv[start:start+ts], label='Predicted Value')\n",
    "plt.plot(y_true_dv[start:start+ts], label='Actual Value')\n",
    "plt.title('Sum of Average Speed Across Network',\n",
    "          fontsize=22, fontweight='bold')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('row')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usd_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
